{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators, Coroutines and all the (other) things\n",
    "\n",
    "Adapted from https://github.com/pymc-devs/pymc4/blob/master/notebooks/pymc4_design_guide.ipynb\n",
    "\n",
    "**This guide is NOT meant to serve as an introduction to probabilistic programming. It is meant for users of pyMC3, switcing to pyMC4, explaining the new model creation and execution paradigm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Theano, reaching the end of its life, pyMC core developers decided to port the framework's backend to Tensorflow Probability (TF Probability/ TPF)(Listen to Weicki's rationale here).  \n",
    "But there are some fundamental design issues which the developers faced. In this document, we go over those problems, and their suggested solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem With TF Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we want from a PPL ?\n",
    "\n",
    "We want to implement a probabilisitc program (think of a Bayesian model that you usually represent in a directed acyclic graph) that does 2 things: \n",
    "\n",
    "1. Forward sampling to get prior (predictive) samples; \n",
    "2. Reverse evaluation on some inputs to compute the log-probability (if we conditioned on the observed, we are evaluating the unnormalized posterior distribution of the unknown random variables). \n",
    "\n",
    "Specifically for computing the log_prob, we need to keep track of the dependency. For example, if we have something simple like `x ~ Normal(0, 1), y ~ Normal(x, 1)`, in the reverse mode (goal 2 from above) we need to swap `x` with the input `x` (either from user or programmatically) to essentially do `log_prob = Normal(0, 1).log_prob(x_input) + Normal(x_input, 1).log_prob(y_input)`.\n",
    "\n",
    "There are a few approaches to this problem. For example, in PyMC3 with a static graph backend (theano), we are able to write things in a declarative way and use the static computational graph to keep track of the dependences. This means we are able to do `log_prob = x.log_prob(x) + y.log_prob(y)`, as the value representation of the random variables `x` and `y` are \"swap-in\" with some input at runtime. With a dynamic graph we run into problems as we write the model in a forward mode -- we essentially lose track of the dependence in the reverse calling mode. We need to either keep track of the graph representation ourselves, or write a function that could be reevaluated to make sure we have the right dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ideally should look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    scale = tfd.HalfCauchy(loc=0, scale=1)\n",
    "    coefs = tfd.Normal(loc=tf.zeros(x.shape[1]), scale=1)\n",
    "    predictions = tfd.Normal(loc=tf.tensordot(x, coefs, axes=1), scale=scale)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(tf.random.normal((100, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this function will not work (you can try it yourself), because there is no random variable concept in `tfp`, meaning that you cannot do `RV_a.log_prob(RV_a)` (yes, just think of Random Variable in a PPL as a tensor/array-like object that you can do computation and a log_prob method that we can evaluate it on itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_w_sample(x):\n",
    "    scale = tfd.HalfCauchy(loc=0, scale=1).sample()\n",
    "    coefs = tfd.Normal(loc=tf.zeros(x.shape[1]), scale=1).sample()\n",
    "    predictions = tfd.Normal(loc=tf.tensordot(x, coefs, axes=1), scale=scale)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w_sample(tf.random.normal((100, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a log_prob from this model also wont work, because the computation is different than the function we have written down above.\n",
    "\n",
    "What we want here is to track function evaluation at runtime, depending on the context (goal 1 or goal 2 from above).\n",
    "\n",
    "The very first way to cope with is was writing a wrapper over a distribution object. This wrapper was intended to catch a call to the distribution and use context to figure out what to do: for goal 1, we draw a sample from a random variable and plug the concrete value into the downstream dependencies; for goal 2 we got the concrete value and evaluate it with the respective random variable, and also plug the concrete value into the downstream dependencies. Here we use Coroutines from Python to have dynamic control flow that could achieve this kind of deferred execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_w_yield(x):\n",
    "    scale = yield tfd.HalfCauchy(loc=0, scale=1)\n",
    "    coefs = yield tfd.Normal(loc=tf.zeros(x.shape[1]), scale=1)\n",
    "    predictions = yield tfd.Normal(loc=tf.tensordot(x, coefs, axes=1), scale=scale)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w_yield(tf.random.normal((100, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we evaluate the model as expected but `yield` allows us to defer the program execution. But before evaluating this function, let's figure out what does yield do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenote -- Primer on Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(message):\n",
    "    print(\"I will yield:\", message)\n",
    "    while True:\n",
    "        yield message\n",
    "    return \"Nothing to yield, Goodbye!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generator(message='generators are cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yielded_value = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yielded_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yielded_value = next(g)\n",
    "yielded_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yielded_value = next(g)\n",
    "yielded_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple infinite loop which will keep returning (rather yielding) the `message` argument passed to it.\n",
    "What if we want it to stop at a specific point. That specific point can either be defined by:\n",
    "\n",
    "1. The number of iteration\n",
    "2. Occurence of a specific (user) input\n",
    "\n",
    "The first goal can be achieved simply by replacing `while True:` by `for i in range(num_iterations):`.\n",
    "\n",
    "\n",
    "The second goal is somewhat trickier. How do we send an input to a already created generator ? \n",
    "Python makes it supereasy by adding a `send` method to generators. And these supercharged generators have a special name - *coroutines*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coroutine(message):\n",
    "    print(\"I will yield:\", message)\n",
    "    sent_value = yield message\n",
    "    while sent_value != 'bye':\n",
    "        print(\"I will yield:\", message)\n",
    "        print(\"And, I was sent:\", sent_value)\n",
    "        sent_value = yield message\n",
    "        \n",
    "    return \"Nothing to yield, Goodbye!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Nothing changes in the definition of a generator and coroutine. The generator `g` we defined has a `send` method attached to it too. The only difference is that in coroutines, the programmer uses the `send` method to perform some compution inside the generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = coroutine(message='coroutines are cooler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st call\n",
    "yielded_value = c.send(None) # or next(c) \n",
    "# assignment doesn't happen on the 1st call, so have to pass `None` as an argument to send\n",
    "# It makes some sense, because if the generator were to yield only one value (which it computed in the function), \n",
    "# there isn't any point passing an input because it will never be used anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yielded_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On 2nd call -- assign this to `sent_value` in 3rd line of `coroutine`\n",
    "# Note: The `yield message` on the RHS won't actually yield anything\n",
    "# As it has already done that when `c.send(None)` was executed\n",
    "\n",
    "yielded_value = c.send('keep going') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yielded_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yielded_value = c.send('bye') \n",
    "yielded_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator and generators\n",
    "def decorator(f):\n",
    "    print('Executing:', f.__name__)\n",
    "    print('-'*30)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        f(*args, **kwargs)\n",
    "    return wrapper    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator\n",
    "def func(msg):\n",
    "    print('Hello, world.', msg)\n",
    "    \n",
    "func(msg=\"I'm decorated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        g = f(kwargs['message'])\n",
    "        try:\n",
    "            l = yield from g\n",
    "            print('done')\n",
    "        except StopIteration as e:\n",
    "            print('Function Finished')\n",
    "        finally:\n",
    "            print(l)\n",
    "        return 'Wrapper finished'\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t\n",
    "def generator(message):\n",
    "    print(\"I will yield:\", message)\n",
    "    result = yield message\n",
    "    result = yield result\n",
    "    return \"Nothing to yield, Goodbye!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = generator(message='generators are cool')\n",
    "next(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.send('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [TD] What has happened here:\n",
    "\n",
    "* we had a simple generator and were able to communicate with it via `send`\n",
    "* after `send` is called (first time requires it to have `None` argument) generator goes to the next `yield` expression and yields what it is asked to yield.\n",
    "* as a return value from `send` we have this exact message from `yield message`\n",
    "* we set the lhs of `response = yield message` with next `send` and no earlier\n",
    "* after generator has no `yield` statements left and finally reaches `return`, it raises `StopIteration` with return value as a first argument\n",
    "\n",
    "Now we are ready to evaluate our model by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments:\n",
    "\n",
    "1. Call `next(g)` instead of `g.send(None)`\n",
    "2. Replace the `return` with `yield`\n",
    "3. Add `sent_value = yield sent_value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Problem using Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = dict(dists=dict(), samples=dict())\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"input\"] = tf.random.normal((3, 10))\n",
    "m = model_w_yield(state[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dist = next(m) # or m.send(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scale_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means, we are here\n",
    "\n",
    "```python\n",
    "def model(x):\n",
    "    scale = yield tfd.HalfCauchy(0, 1) # <--- HERE\n",
    "    coefs = yield tfd.Normal(tf.zeros(x.shape[1]), 1, )\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do with this distribution? We can choose forward sampling (in this case we sample from the state-less distribution `HalfCauchy(0, 1)`). But we need it to be used by user seamlessly later on regardless of the context (goal 1 or 2 above). On the model side, we need to store intermediate values and its associated distributions (hey! that's a random variable!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert scale_dist.name not in state[\"dists\"]\n",
    "state[\"samples\"][scale_dist.name] = scale_dist.sample()\n",
    "state[\"dists\"][scale_dist.name] = scale_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_dist = m.send(state[\"samples\"][scale_dist.name]) \n",
    "# assingment scale = ... and yield tfd.Normal(...) happens simultaneously in this call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coefs_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def model(x):\n",
    "    scale = yield tfd.HalfCauchy(0, 1)\n",
    "    coefs = yield tfd.Normal(tf.zeros(x.shape[1]), 1, ) # <--- WE ARE NOW HERE\n",
    "    predictions = yield tfd.Normal(tf.linalg.matvec(x, coefs), scale)\n",
    "    return predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert coefs_dist.name not in state[\"dists\"]\n",
    "state[\"samples\"][coefs_dist.name] = coefs_dist.sample()\n",
    "state[\"dists\"][coefs_dist.name] = coefs_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dist = m.send(state[\"samples\"][coefs_dist.name]) \n",
    "# assingment coefs = ... and yield tfd.Normal(tf.linalg...) happens simultaneously in this call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def model(x):\n",
    "    scale = yield tfd.HalfCauchy(0, 1)\n",
    "    coefs = yield tfd.Normal(tf.zeros(x.shape[1]), 1, )\n",
    "    predictions = yield tfd.Normal(tf.linalg.matvec(x, coefs), scale) # <--- NOW HERE\n",
    "    return predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now facing predictive distribution. Here we have several options:\n",
    "* sample from it: we get prior predictive\n",
    "* set custom values instead of sample, essentially conditioning on data. We might be interested in this to compute unnormalized posterior\n",
    "* replace it with another distribution, arbitrary magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert preds_dist.name not in state[\"dists\"]\n",
    "state[\"samples\"][preds_dist.name] = tf.zeros(preds_dist.batch_shape)\n",
    "state[\"dists\"][preds_dist.name] = preds_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gotcha, we found duplicated names in our toy graphical model. We can easily tell our user to rewrite the model to get rid of duplicate names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.throw(RuntimeError(\n",
    "    \"We found duplicate names in your cool model: {}, \"\n",
    "    \"so far we have other variables in the model, {}\".format(\n",
    "        preds_dist.name, set(state[\"dists\"].keys()), \n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good thing is that we *communicate* with user, and can give meaningful exceptions with few pain.\n",
    "\n",
    "The correct model should look like this:\n",
    "\n",
    "```python\n",
    "def model(x):\n",
    "    scale = yield tfd.HalfCauchy(0, 1)\n",
    "    coefs = yield tfd.Normal(tf.zeros(x.shape[1]), 1, )\n",
    "    predictions = yield tfd.Normal(tf.linalg.matvec(x, coefs), scale, name=\"Normal_1\") # <--- HERE we asked out user to change the name\n",
    "    return predictions\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set all the names according to the new model and interact with user again using the same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our generator is now at the end of its execution - we can't interact with it any more. Let's create a new one and reevaluate with same sampled values (A hint how to get the desired `logp` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    scale = yield tfd.HalfCauchy(0, 1)\n",
    "    coefs = yield tfd.Normal(tf.zeros(x.shape[1]), 1, )\n",
    "    predictions = yield tfd.Normal(tf.linalg.matvec(x, coefs), scale, name=\"Normal_1\") # <--- HERE we asked out user to change the name\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model(state[\"input\"])\n",
    "print(m.send(None))\n",
    "print(m.send(state[\"samples\"][\"HalfCauchy\"]))\n",
    "print(m.send(state[\"samples\"][\"Normal\"]))\n",
    "try:\n",
    "    m.send(tf.zeros(state[\"input\"].shape[0]))\n",
    "except StopIteration as e:\n",
    "    stop_iteration = e\n",
    "else:\n",
    "    raise RuntimeError(\"No exception met\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_iteration.args[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of returning some value in the last `send`, generator raises `StopIteration` because it is exhausted and reached the `return` statement (no more `yield` met). As explained (and checked here) in [PEP0342](https://www.python.org/dev/peps/pep-0342/), we have a return value inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate the process above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all are lazy humans and cant stand doing repetitive things. In our model evaluation we followed pretty simple rules:\n",
    "* asserting name is not used\n",
    "* checking if we should sample or place a specific value instead\n",
    "* recording distributions and samples\n",
    "\n",
    "Next step is to make a function that does all this instead of us. In this tutorial let's keep it simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact(gen, state):\n",
    "    control_flow = gen()\n",
    "    return_value = None\n",
    "    while True:\n",
    "        try:\n",
    "            dist = control_flow.send(return_value)\n",
    "            if dist.name in state[\"dists\"]:\n",
    "                control_flow.throw(RuntimeError(\n",
    "                    \"We found duplicate names in your cool model: {}, \"\n",
    "                    \"so far we have other variables in the model, {}\".format(\n",
    "                        dist.name, set(state[\"dists\"].keys()), \n",
    "                    )\n",
    "                ))\n",
    "            if dist.name in state[\"samples\"]:\n",
    "                return_value = state[\"samples\"][dist.name]\n",
    "            else:\n",
    "                return_value = dist.sample()\n",
    "                state[\"samples\"][dist.name] = return_value\n",
    "            state[\"dists\"][dist.name] = dist\n",
    "        except StopIteration as e:\n",
    "            if e.args:\n",
    "                return_value = e.args[0]\n",
    "            else:\n",
    "                return_value = None\n",
    "            break\n",
    "    return return_value, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation assumes no arg generator, we make things just simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, state = interact(lambda: model(tf.random.normal((3, 10))), state=dict(dists=dict(), samples=dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get all the things as expected. To calculate `logp` you just iterate over distributions and match them with the correspondig values. But let's dive deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One level deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the motivating example from [PR#125](https://github.com/pymc-devs/pymc4/pull/125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Horseshoe(mu=0, tau=1., s=1., name=None):\n",
    "    with tf.name_scope(name):\n",
    "        scale = yield tfd.HalfCauchy(0, s, name=\"scale\")\n",
    "        noise = yield tfd.Normal(0, tau, name=\"noise\")\n",
    "        return scale * noise + mu\n",
    "\n",
    "def linreg(x):\n",
    "    scale = yield tfd.HalfCauchy(0, 1, name=\"scale\")\n",
    "    coefs = yield Horseshoe(tf.zeros(x.shape[1]), name=\"coefs\")\n",
    "    predictions = yield tfd.Normal(tf.linalg.matvec(x, coefs), scale, name=\"predictions\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = Horseshoe(name='h1')\n",
    "# next(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, state = interact(lambda: linreg(tf.random.normal((3, 10))), state=dict(dists=dict(), samples=dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooups, we have a type error. What we want is a nested model, but nesting models is something different from a plain generator. As we have our model being a generator itself, the return value of `Horseshoe(tf.zeros(x.shape[1]), name=\"coefs\")` is a generator. Of course this generator has no name attribute. Okay, we can ask user to use `yield from` construction to generate from the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_ugly(x):\n",
    "    scale = yield tfd.HalfCauchy(0, 1, name=\"scale\")\n",
    "    coefs = yield from Horseshoe(tf.zeros(x.shape[1]), name=\"coefs\")\n",
    "    predictions = yield tfd.Normal(tf.linalg.matvec(x, coefs), scale, name=\"predictions\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, state = interact(lambda: linreg_ugly(tf.random.normal((3, 10))), state=dict(dists=dict(), samples=dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we passed this thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"dists\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got nesting models working, but it requires `yield from`. This is UGLY and potentially confusing for user. Fortunately, we can rewrite out `interact` function to accept nested models in a few lines, and let the Python do the task for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "def interact_nested(gen, state):\n",
    "    # for now we should check input type\n",
    "    if not isinstance(gen, types.GeneratorType):\n",
    "        control_flow = gen()\n",
    "    else:\n",
    "        control_flow = gen\n",
    "\n",
    "    return_value = None\n",
    "    while True:\n",
    "        try:\n",
    "            dist = control_flow.send(return_value)\n",
    "            # this makes nested models possible\n",
    "            if isinstance(dist, types.GeneratorType):\n",
    "                return_value, state = interact_nested(dist, state)\n",
    "                # ^ in a few lines of code, go recursive\n",
    "            else:\n",
    "                if dist.name in state[\"dists\"]:\n",
    "                    control_flow.throw(RuntimeError(\n",
    "                        \"We found duplicate names in your cool model: {}, \"\n",
    "                        \"so far we have other variables in the model, {}\".format(\n",
    "                            dist.name, set(state[\"dists\"].keys()), \n",
    "                        )\n",
    "                    ))\n",
    "                if dist.name in state[\"samples\"]:\n",
    "                    return_value = state[\"samples\"][dist.name]\n",
    "                else:\n",
    "                    return_value = dist.sample()\n",
    "                    state[\"samples\"][dist.name] = return_value\n",
    "                state[\"dists\"][dist.name] = dist\n",
    "        except StopIteration as e:\n",
    "            if e.args:\n",
    "                return_value = e.args[0]\n",
    "            else:\n",
    "                return_value = None\n",
    "            break\n",
    "    return return_value, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember we had problems here:\n",
    "```python\n",
    "preds, state = interact(lambda: linreg(tf.random.normal((3, 10))), state=dict(dists=dict(), samples=dict()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we can specify the observed variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, state = interact_nested(lambda: linreg(tf.random.normal((3, 10))), state=dict(dists=dict(), samples={\"predictions/\":tf.zeros(3)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"dists\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"samples\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we've finished the central idea behind PyMC4 core engine. There is some extra stuff to do to make `evaluate_nested` really powerful\n",
    "\n",
    "* resolve transforms\n",
    "* resolve reparametrizations\n",
    "* variational inference\n",
    "* better error messages\n",
    "* lazy returns in posterior predictive mode\n",
    "\n",
    "Some of this functionality may be found in the corresponding [PR#125](https://github.com/pymc-devs/pymc4/pull/125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_decorator(gen):\n",
    "    \n",
    "    state = dict(dists=dict(), samples=dict())\n",
    "    \n",
    "    def interact(x):\n",
    "        control_flow = gen\n",
    "        return_value = None\n",
    "        while True:\n",
    "            try:\n",
    "                dist = control_flow.send(return_value)\n",
    "                if dist.name in state[\"dists\"]:\n",
    "                    control_flow.throw(RuntimeError(\n",
    "                        \"We found duplicate names in your cool model: {}, \"\n",
    "                        \"so far we have other variables in the model, {}\".format(\n",
    "                            dist.name, set(state[\"dists\"].keys()), \n",
    "                        )\n",
    "                    ))\n",
    "                if dist.name in state[\"samples\"]:\n",
    "                    return_value = state[\"samples\"][dist.name]\n",
    "                else:\n",
    "                    return_value = dist.sample()\n",
    "                    state[\"samples\"][dist.name] = return_value\n",
    "                state[\"dists\"][dist.name] = dist\n",
    "            except StopIteration as e:\n",
    "                if e.args:\n",
    "                    return_value = e.args[0]\n",
    "                else:\n",
    "                    return_value = None\n",
    "                break\n",
    "        return return_value, state\n",
    "    \n",
    "    return interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model_decorator\n",
    "def linreg_ugly(x, state):\n",
    "    scale = yield tfd.HalfCauchy(0, 1, name=\"scale\")\n",
    "    coefs = yield from Horseshoe(tf.zeros(x.shape[1]), name=\"coefs\")\n",
    "    predictions = yield tfd.Normal(tf.linalg.matvec(x, coefs), scale, name=\"predictions\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = lambda: linreg_ugly(tf.random.normal((3, 10)))\n",
    "# preds, state = interact(gen, state=dict(dists=dict(), samples=dict()))\n",
    "x = tf.random.normal((3, 10))\n",
    "preds, state = linreg_ugly(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://www.dabeaz.com/coroutines/\n",
    "* https://stackoverflow.com/a/19302700\n",
    "* http://simeonvisser.com/posts/python-3-using-yield-from-in-generators-part-1.html\n",
    "* https://stackoverflow.com/a/26109157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
